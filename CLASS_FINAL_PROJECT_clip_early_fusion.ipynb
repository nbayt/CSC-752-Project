{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19112a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nickb\\anaconda3\\envs\\csc-752-final-project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\nickb\\anaconda3\\envs\\csc-752-final-project\\lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3691c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = \"C:\\\\Users\\\\alan\\\\Medical Image Project\\\\combine_data\\\\BrEaST-Lesions-USG-clinical-data-Dec-15-2023.xlsx\"\n",
    "file_path = \"BrEaST-Lesions-USG-clinical-data-Dec-15-2023.xlsx\"\n",
    "text_data = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b114c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 21)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39446f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickb\\AppData\\Local\\Temp\\ipykernel_17604\\832268161.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_text_data['combined_text'] = filtered_text_data[text_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# filter out normal rows\n",
    "filtered_text_data = text_data[(text_data['Classification'] == 'benign') | (text_data['Classification'] == 'malignant')]\n",
    "\n",
    "# combine the relevant columns into a single text field per case\n",
    "text_columns = [col for col in filtered_text_data.columns if col not in ['Image_filename', 'Classification', \"Mask_tumor_filename\",\"Mask_other_filename\", \"Pixel_size\", \"Verification\",\"BIRADS\"]]\n",
    "filtered_text_data['combined_text'] = filtered_text_data[text_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eaf3519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nickb\\anaconda3\\envs\\csc-752-final-project\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nickb\\anaconda3\\envs\\csc-752-final-project\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nickb\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\nickb\\anaconda3\\envs\\csc-752-final-project\\lib\\site-packages\\transformers\\modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "\n",
    "text_inputs = processor(text=filtered_text_data[\"combined_text\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a09be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = model.get_text_features(**text_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80eeba89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([252, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deece36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image feature extraction\n",
    "def get_image_embedding(image_path, processor, model):\n",
    "    # Load and process the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate embedding\n",
    "    with torch.no_grad():  # Ensure no gradients are calculated\n",
    "        image_embedding = model.get_image_features(**inputs)\n",
    "    \n",
    "    return image_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f1a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold image embeddings\n",
    "image_embeddings = []\n",
    "\n",
    "# Base path for images\n",
    "base_path = \"C:\\\\Users\\\\alan\\\\Medical Image Project\\\\combine_data\"\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for filename in filtered_text_data['Image_filename']:\n",
    "    image_path = os.path.join(base_path, filename)\n",
    "    embedding = get_image_embedding(image_path, processor, model)\n",
    "    image_embeddings.append(embedding)\n",
    "\n",
    "# Convert the list of embeddings into a tensor (or any format you prefer)\n",
    "image_embeddings_tensor = torch.stack(image_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d167e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert both embedding into same demision\n",
    "image_embeddings_tensor = image_embeddings_tensor.squeeze(-1)  # Squeeze the last dimension if it's extraneous\n",
    "\n",
    "image_embeddings_tensor = image_embeddings_tensor.view(image_embeddings_tensor.shape[0], -1)  # Flatten to [number_of_samples, image_feature_size * some_other_dimension]\n",
    "\n",
    "# Now try concatenating again\n",
    "combined_embeddings = torch.cat((text_embeddings, image_embeddings_tensor), dim=1)\n",
    "print(text_embeddings.shape)\n",
    "print(image_embeddings_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(combined_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9b2cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label\n",
    "labels = filtered_text_data['Classification'].values\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)  # Converts labels to numerical format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "\n",
    "# Convert it to a NumPy array\n",
    "X = combined_embeddings.detach().numpy()\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dropout(0.2),                 # Dropout layer for regularization\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),         # BatchNormalization layer for normalization\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with validation data\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.4, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\\nTest Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Training and Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training and Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44974fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd19ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4824bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc-752-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
